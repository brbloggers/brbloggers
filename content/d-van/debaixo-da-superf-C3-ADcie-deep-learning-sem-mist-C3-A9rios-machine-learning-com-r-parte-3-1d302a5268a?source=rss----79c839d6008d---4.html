<div><div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p id="0f1c" class="graf graf--p graf-after--h3">Nos textos anteriores (1&#xAA; parte&#x200A;&#x2014;&#x200A;<a href="https://d-van.org/predizendo-as-mortes-do-game-of-thrones-introdu%C3%A7%C3%A3o-a-machine-learning-com-r-parte-1-19c09a93f8a3" class="markup--anchor markup--p-anchor">link</a>), mostramos o funcionamento de um classificador simples e usamos (<a href="https://d-van.org/conhecendo-os-pacotes-populares-introdu%C3%A7%C3%A3o-a-machine-learning-com-r-parte-2-8bff273fbb76" class="markup--anchor markup--p-anchor">link</a>) um pacote popular para ilustrar a configura&#xE7;&#xE3;o, treinamento e avalia&#xE7;&#xE3;o do modelo (Support Vector Machine).</p><p id="8499" class="graf graf--p graf-after--p">Um mal entendido envolvendo<em class="markup--em markup--p-em"> deep learning</em> &#xE9; de que produzimos uma caixa preta, &#xFA;til por&#xE9;m inacess&#xED;vel. Vamos entender como funcionam redes profundas e de onde surge essa confus&#xE3;o.</p><figure id="b1cb" class="graf graf--figure graf-after--h4"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*uz0Y8_dNGXxh1En_QqwZHA.png"><figcaption class="imageCaption">Exemplo de &#x201C;1&#x201D; em letra cursiva e sua representa&#xE7;&#xE3;o numa matriz 2x2. <a href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/" class="markup--anchor markup--figure-anchor">http://colah.github.io/posts/2014-10-Visualizing-MNIST/</a></figcaption></figure><p id="e48e" class="graf graf--p graf-after--figure">Podemos representar imagens usando matrizes, como na figura acima. O monitor l&#xEA; cada n&#xFA;mero e ativa o ponto brilhante correspondente na tela, criando a ilus&#xE3;o de imagens e filmes.</p><p id="0fdb" class="graf graf--p graf-after--p">Implementamos um classificador simples (<em class="markup--em markup--p-em">Support Vector Machine</em>, SVM), que l&#xEA; uma imagem, como o 1 acima, na forma de matriz. Usando uma fun&#xE7;&#xE3;o (<em class="markup--em markup--p-em">kernel function</em>), que aceita esse input e considera pesos internos (w), gerando scores empregados nas predi&#xE7;&#xF5;es.</p><figure id="12cf" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*GFQr27jiqY0jtxpGKdsZTQ.png"></figure><p id="4234" class="graf graf--p graf-after--figure">Esse kernel pode ser simples, com apenas combina&#xE7;&#xF5;es lineares, ou mais complexo, com outras fun&#xE7;&#xF5;es <a href="https://en.wikipedia.org/wiki/Radial_basis_function_kernel" class="markup--anchor markup--p-anchor">(e.g. RBF)</a>.</p><p id="6fe8" class="graf graf--p graf-after--h3">Com o aprendizado atrav&#xE9;s de exemplos, otimizamos otimizamos nosso classificador (mudando pesos W) para minimizar a perda, erro, usando aproxima&#xE7;&#xF5;es(e.g: <a href="https://xcorr.net/2014/01/23/adagrad-eliminating-learning-rates-in-stochastic-gradient-descent/" class="markup--anchor markup--p-anchor">Adagrad</a>). A fun&#xE7;&#xE3;o de perda &#xE9; menor quando temos pontua&#xE7;&#xF5;es (votos) maiores para as classes certas.</p><p id="da71" class="graf graf--p graf-after--p">SVMs t&#xEA;m bom desempenho em diversas estruturas de dados, especialmente quando a arquitetura &#xE9; otimizada por um usu&#xE1;rio experiente. Onde entram as redes neurais?</p><figure id="01a9" class="graf graf--figure graf-after--p"><img class="graf-image" alt="Image result for signal vs noise beautiful" src="https://cdn-images-1.medium.com/max/1600/1*bpxmMpSKLs86DZWiP-_mnw@2x.png"></figure><p id="479f" class="graf graf--p graf-after--h3">As vers&#xF5;es reais da maioria dos conceitos criados por seres humanos n&#xE3;o s&#xE3;o id&#xEA;nticas umas &#xE0;s outras. Em outras palavras, n&#xE3;o existe um conjunto r&#xED;gido de regras para classificarmos a maior parte das entidades ao nosso redor.</p><p id="bce5" class="graf graf--p graf-after--p">Muitas entidades s&#xE3;o diferentes, por&#xE9;m similares o suficiente para pertencer a uma mesma categoria.</p><figure id="e2ad" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/0*OBb0FiADapopd5HG.jpg"><figcaption class="imageCaption">Todos s&#xE3;o naturalmente reconhecidos como felinos, mas apresentam varia&#xE7;&#xF5;es de tamanho, cor e propor&#xE7;&#xE3;o em todo o corpo. <a href="http://voices.nationalgeographic.org/files/2015/12/Wild-Cat-Species.jpg" class="markup--anchor markup--figure-anchor">http://voices.nationalgeographic.org/files/2015/12/Wild-Cat-Species.jpg</a></figcaption></figure><p id="43ee" class="graf graf--p graf-after--figure">Esse &#xE9; um problema interessante e antigo. Alguns fil&#xF3;sofos acreditam que abstra&#xE7;&#xF5;es humanas s&#xE3;o inst&#xE2;ncias de um conceito mais gen&#xE9;rico: mapas biol&#xF3;gicos contidos em redes neuronais (Paul Churchland<a href="https://mitpress.mit.edu/books/platos-camera" class="markup--anchor markup--p-anchor">, Plato&#x2019;s Camera</a>).</p><p id="f56d" class="graf graf--p graf-after--p">Esses mapas est&#xE3;o associados de forma hierarquizada. Numerosos padr&#xF5;es em n&#xED;veis inferiores e um n&#xFA;mero menor em camadas superiores.</p><p id="02f4" class="graf graf--p graf-after--p">No caso da vis&#xE3;o, neur&#xF4;nios superficiais captam pontos luminosos. O padr&#xE3;o de ativa&#xE7;&#xE3;o sensorial enviado ao c&#xF3;rtex visual prim&#xE1;rio &#xE9; o primeiro mapa, que &#xE9; torcido e filtrado caminho cima.</p><figure id="9894" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/0*VIuKhQ0U03EfnpUo."><figcaption class="imageCaption"><em class="markup--em markup--figure-em">Resposta a est&#xED;mulos visuais em V1 de Macaca fascicularis </em><a href="http://www.jneurosci.org/content/32/40/13971" class="markup--anchor markup--figure-anchor">http://www.jneurosci.org/content/32/40/13971</a></figcaption></figure><p id="4633" class="graf graf--p graf-after--figure">Neur&#xF4;nios intermedi&#xE1;rios possuem configura&#xE7;&#xF5;es que identificam caracter&#xED;sticas simples: olhos e subcomponentes da face. Por fim, temos camadas mais <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">profundas, </em></strong>ligadas a abstra&#xE7;&#xF5;es.</p><figure id="f379" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*C17gQkpeUaym_NbiAKsheA.png"><figcaption class="imageCaption">Retirado de: <a href="https://www.youtube.com/watch?v=SeyIg6ArS4Y" class="markup--anchor markup--figure-anchor">https://www.youtube.com/watch?v=SeyIg6ArS4Y</a></figcaption></figure><p id="5bc3" class="graf graf--p graf-after--h3">Um classificador deve capturar essa estrutura abstrata a partir de modelos matem&#xE1;ticos trat&#xE1;veis. Para examinarmos esse aspecto, usemos um exemplo. O gr&#xE1;fico abaixo representa milhares de amostras com: (1) a curva di&#xE1;ria natural de um horm&#xF4;nio (em vermelho) e a curva sob uso de esteroides anabolizantes (azul).</p><figure id="2f23" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*hEzMB4MaAviB5MHkAr0-vw.png"><figcaption class="imageCaption">Exemplo inspirado no texto de Chris Olah (<a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" class="markup--anchor markup--figure-anchor">http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/</a>)</figcaption></figure><p id="8622" class="graf graf--p graf-after--figure">Como hipot&#xE9;ticos membros de uma comit&#xEA; atl&#xE9;tico, nosso objetivo aqui &#xE9;, dada uma amostra, saber se o atleta est&#xE1; sob efeito de esteroides.</p><p id="f051" class="graf graf--p graf-after--p">Quando experimentamos, normalmente haver&#xE1; ru&#xED;dos (erros) na medida e receberemos medi&#xE7;&#xF5;es imprecisas da curva. Varia&#xE7;&#xF5;es na dieta daquele dia, mic&#xE7;&#xF5;es, sudorese, stress e outros fatores.</p><p id="1f88" class="graf graf--p graf-after--p">Usamos <strong class="markup--strong markup--p-strong">o tempo (t, </strong>eixo horizontal) e n&#xED;vel hormonal (<strong class="markup--strong markup--p-strong">&#x3B2;, </strong>eixo vertical).&#xA0;<br>Numa regress&#xE3;o log&#xED;stica simples, fazemos essa classifica&#xE7;&#xE3;o com base nas probabilidades de uma fun&#xE7;&#xE3;o sigmoide. Temos uma probabilidade (valor entre 0 e 1).</p><p id="2826" class="graf graf--p graf-after--p">P(h,&#x3B2;) = 1/(1+exp-(i+t*h+&#x3B2;*y+&#x3B5;)).<br>&#x3B5; representa o erro e i &#xE9; uma constante. Em uma linha do R:</p><pre id="0dc1" class="graf graf--pre graf-after--p">logist.fit &lt;- glm(type_dic ~ beta + tempo,               family=binomial,data=inv.ds)<br># Criei 1000 time-points ao longo do dia.</pre><p id="f794" class="graf graf--p graf-after--pre">A vantagem de usar essa modelagem &#xE9; que temos uma rela&#xE7;&#xE3;o direta entre o inverso dessa fun&#xE7;&#xE3;o (P^(-1), &#x201C;logito&#x201D;) e a combina&#xE7;&#xE3;o linear dos nossos par&#xE2;metros:<br><strong class="markup--strong markup--p-strong">logit (P(x))=i+t*x+&#x3B2;*y+&#x3B5;</strong></p><p id="c378" class="graf graf--p graf-after--p">Em outras palavras, o processo de estima&#xE7;&#xE3;o &#xE9; parecido com o da regress&#xE3;o linear, que &#xE9; facilmente trat&#xE1;vel. Outra consequ&#xEA;ncia &#xE9; que assumimos que a distin&#xE7;&#xE3;o entre classes (<em class="markup--em markup--p-em">com base no logito, log odds</em>) pode ser dada por um limite. Este tem uma rela&#xE7;&#xE3;o linear com nossas vari&#xE1;veis. Estimamos a magnitude e o sentido dessas rela&#xE7;&#xF5;es pelos par&#xE2;metros da regress&#xE3;o.</p><figure id="35c3" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*-yWax6CWeQBKpVVFshk89g.png"><figcaption class="imageCaption">Podemos imaginar que o log odds (z, eixo vertical) cresce linearmente com uma combina&#xE7;&#xE3;o de duas variaveis (x e y). Notem que a superf&#xED;cie definida pelo nossa equa&#xE7;&#xE3;o/modelo &#xE9; um plano. z = 3 + 3x + 2y. Plotado no Wolfram&#xA0;Alpha</figcaption></figure><p id="5ab3" class="graf graf--p graf-after--figure">Estimamos qual seria a posi&#xE7;&#xE3;o na reta dada por aquela medida e usamos um limite de decis&#xE3;o (<em class="markup--em markup--p-em">decision boundary) </em>linear. Voltando ao nosso exemplo, seria dif&#xED;cil capturar as diferen&#xE7;as usando apenas esta estrat&#xE9;gia.</p><figure id="d9ed" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/0*pqQRrIXlsUTY6upr.png"><figcaption class="imageCaption">Acima, um neur&#xF4;nio sigmoide, que equivale &#xE0; regress&#xE3;o log&#xED;stica. &#xC9; como o plano anterior, mas visto de cima, dividimos ele em duas regi&#xF5;es para classfica&#xE7;&#xE3;o. <a href="http://colah.github.io/posts/2015-01-Visualizing-Representations/" class="markup--anchor markup--figure-anchor">http://colah.github.io/posts/2015-01-Visualizing-Representations/</a></figcaption></figure><p id="18ba" class="graf graf--p graf-after--figure">Por que? O classificador linear otimiza suas respostas levando em conta apenas o valor absoluto da medida hormonal. Isto &#xE9;, valores acima de um limite ser&#xE3;o considerados dopping, n&#xE3;o considerando hor&#xE1;rio. Matematicamente, o coeficiente para o tempo foi ajustado em 0. Mudar isso tornaria a reta divis&#xF3;ria inclinada em rela&#xE7;&#xE3;o ao eixo x, piorando a classifica&#xE7;&#xE3;o.</p><p id="dc41" class="graf graf--p graf-after--p">Podemos verificar isso diretamente atrav&#xE9;s dos par&#xE2;metros estimados em nosso modelo de regress&#xE3;o.</p><pre id="ce5f" class="graf graf--pre graf-after--p">&gt; summary(logist.fit)</pre><pre id="f9c9" class="graf graf--pre graf-after--pre">Call:  glm(formula = type_dic ~ beta + tempo, family = binomial, data = inv.ds)</pre><pre id="6210" class="graf graf--pre graf-after--pre">Coefficients:<br>  (Intercept)     beta        <strong class="markup--strong markup--pre-strong">tempo </strong> <br>-0.8752803   -3.6195723   <strong class="markup--strong markup--pre-strong">-0.0001221 # Pr&#xF3;ximo a zero</strong></pre><pre id="f7df" class="graf graf--pre graf-after--pre">Degrees of Freedom: 999 Total (i.e. Null);  997 Residual<br>Null Deviance:     1386 <br>Residual Deviance: 774.4  AIC: 780.4<br>&gt; prob=predict(logist.fit,type=c(&quot;response&quot;))<br>&gt; inv.ds$prob=prob<br>&gt; curve &lt;- roc(type_dic ~ prob, data = inv.ds)<br>&gt; curve<br><br>Call:<br>  roc.formula(formula = type_dic ~ prob, data = inv.ds)</pre><pre id="c17b" class="graf graf--pre graf-after--pre">Data: prob in 500 controls (type_dic 0) &lt; 500 cases (type_dic 1).<br>Area under the curve: 0.8767</pre><p id="2240" class="graf graf--p graf-after--h3">A solu&#xE7;&#xE3;o &#xE9; introduzir termos polinomiais de grau mais alto (x&#xB2;,x&#xB3;&#x2026;), intera&#xE7;&#xF5;es ou usar fun&#xE7;&#xF5;es mais complexas. A&#xED; corremos o risco de realizar <strong class="markup--strong markup--p-strong">sobre ajuste</strong>. Deixar o sinal dos confundir e fazer um modelo complexo que n&#xE3;o funciona em novos exemplos.</p><p id="d0c5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">E o que acontece se conectarmos classificadores simples hierarquicamente?</strong></p><p id="4e0b" class="graf graf--p graf-after--p">A resposta de uma unidade &#xE9; usada como a entrada de outras. Quando processamos o sinal em etapas, cada camada modifica os dados para as camadas posteriores, transformando e filtrando/dando forma.</p><p id="31e7" class="graf graf--p graf-after--p">As camadas intermedi&#xE1;rias permitem a transforma&#xE7;&#xE3;o gradual do sinal, e o sistema acerta usando apenas dois classificadores simples (<a href="https://en.wikipedia.org/wiki/Sigmoid_function" class="markup--anchor markup--p-anchor">sigmoides</a>). No exemplo acima, temos uma camada de 2 neur&#xF4;nios entre o input e o output.</p><figure id="338d" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/0*bmzRtQU8C4BmBTLC.png"><figcaption class="imageCaption"><a href="http://colah.github.io/posts/2015-01-Visualizing-Representations/" class="markup--anchor markup--figure-anchor">http://colah.github.io/posts/2015-01-Visualizing-Representations/</a></figcaption></figure><p id="2d32" class="graf graf--p graf-after--figure">Agora, a primeira camada (hidden) modifica a entrada com duas unidades sigmoides e a segunda camada pode classificar corretamente usando apenas uma reta, algo que era imposs&#xED;vel antes.</p><p id="c120" class="graf graf--p graf-after--p">Em tese, esse modelo pode capturar melhor as caracter&#xED;sticas que geraram os dados (flutua&#xE7;&#xE3;o hormonal ao longo do dia).</p><p id="0c8f" class="graf graf--p graf-after--h3">Notem que o diagrama acima lembra uma rede neural. Esse tipo de classificador foi inspirado na organiza&#xE7;&#xE3;o microsc&#xF3;pica de neur&#xF4;nios reais e acredita-se que seu funcionamento seja de alguma forma an&#xE1;logo. A arquitetura de redes convolucionais (<em class="markup--em markup--p-em">convolutional neural networks</em>), estado da arte em reconhecimento de imagens, foi inspirada no <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1557912/" class="markup--anchor markup--p-anchor">c&#xF3;rtex visual de mam&#xED;feros</a>.</p><p id="63d2" class="graf graf--p graf-after--p">Outros modelos bio inspirados (Spiking neural networks, LTSMs&#x2026;) apresentam desempenhos in&#xE9;ditos para tarefas complexas e pouco estruturadas, como reconhecimento de voz e tradu&#xE7;&#xE3;o de textos.</p><p id="d422" class="graf graf--p graf-after--p">A teoria mais aceita &#xE9; de que o maquin&#xE1;rio neural dos animais foi desenhado por processos evolutivos, como a sele&#xE7;&#xE3;o natural. Assim, apresenta coloridas formas de complexidade a depender da tarefa desempenhada.</p><figure id="c9a1" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*hWYpsxWOj9IdpLQeaUOE8w.png"><figcaption class="imageCaption">Como podemos ver, as redes biol&#xF3;gicas s&#xE3;o complexas, com at&#xE9; dezenas de bilh&#xF5;es de unidades de processamento paralelas conectadas. Zona destacada possui grafo isomorfo ao descrito no texto. Modificado de <a href="http://www.rzagabe.com/2014/11/03/an-introduction-to-artificial-neural-networks.html" class="markup--anchor markup--figure-anchor">http://www.rzagabe.com/2014/11/03/an-introduction-to-artificial-neural-networks.html</a></figcaption></figure><p id="cf58" class="graf graf--p graf-after--figure">Nos modelos profundos (<strong class="markup--strong markup--p-strong">deep</strong>) de reconhecimento de rosto, neur&#xF4;nios de camadas superficiais capturam bordas, &#xE2;ngulos e v&#xE9;rtices, camadas intermedi&#xE1;rias detectam presen&#xE7;a de olhos, boca, nariz. Por fim, camadas ao final da arquitetura decidem se &#xE9; um rosto ou n&#xE3;o e a quem ele pertence.</p><p id="114b" class="graf graf--p graf-after--h3">Podemos demonstrar formalmente que uma rede neural com apenas uma camada interna &#xE9; capaz de aproximar <strong class="markup--strong markup--p-strong">qualquer </strong>fun&#xE7;&#xE3;o. A <a href="http://neuralnetworksanddeeplearning.com/chap4.html" class="markup--anchor markup--p-anchor">prova </a>n&#xE3;o &#xE9; l&#xE1; essas coisas, j&#xE1; que, no fundo o que fazemos &#xE9; criar uma tabela de consulta (lookup table) para os valores de entrada e sa&#xED;da usando os neur&#xF4;nios.</p><p id="abf5" class="graf graf--p graf-after--p">Na pr&#xE1;tica, &#xE9; dif&#xED;cil obter boas performances. T&#xE3;o dif&#xED;cil que redes neurais passaram d&#xE9;cadas esquecidas. Se voc&#xEA; rodar o modelo abaixo, baseado no nosso exemplo, ver&#xE1; que a acur&#xE1;cia &#xE9; pr&#xF3;xima da regress&#xE3;o log&#xED;stica. &#xC9; necess&#xE1;rio algum conhecimento e tempo para afinar os detalhes.</p><p id="bf06" class="graf graf--p graf-after--p">Normalmente, depende da qualidade e da quantidade dos dados.</p><pre id="0146" class="graf graf--pre graf-after--p"># Neural Net para o exemplo<br>library(deepnet)<br>inv.ds$tempo.norm &lt;- normalize(inv.ds$tempo)</pre><pre id="9cfe" class="graf graf--pre graf-after--pre">deep.log.dbn &lt;- dbn.dnn.train(<br>  x=as.matrix(inv.ds[,c(&quot;beta&quot;,&quot;tempo.norm&quot;)]),<br>  y=as.numeric(as.character(inv.ds$type_dic)),<br>  hidden = c(2), activationfun = &quot;sigm&quot;,<br>  learningrate=2.65, momentum=0.85, learningrate_scale=1,<br>  output = &quot;sigm&quot;, numepochs=3, batchsize= 11)</pre><p id="02bc" class="graf graf--p graf-after--pre">As redes neurais passaram algum tempo esquecidas, at&#xE9; que <a href="http://people.idsia.ch/~juergen/who-invented-backpropagation.html" class="markup--anchor markup--p-anchor">algumas reviravoltas</a> &#xB9; permitiram o treinamento eficaz delas redes. Algoritmos para melhorar o treinamento, assim como arquiteturas econ&#xF4;micas ou especialmente boas em determinadas tarefas.</p><p id="7c69" class="graf graf--p graf-after--p">Al&#xE9;m disso, o uso de processadores gr&#xE1;ficos (GPU), desenhados para as opera&#xE7;&#xF5;es de &#xE1;lgebra linear que discutimos (com matrizes) permitiu treinar em um volume maior de dados.</p><p id="0591" class="graf graf--p graf-after--h3">Uma vez que o texto &#xE9; sobre <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">deep leaning, </em></strong>precisamos<strong class="markup--strong markup--p-strong"> </strong>falar de <em class="markup--em markup--p-em">backpropagation&#xA0;</em>.</p><p id="6b91" class="graf graf--p graf-after--p">Como vimos nas partes 1 e 2, o treinamento consiste em ajustar os pesos W do classificador (SVM) para minimizar a fun&#xE7;&#xE3;o que calcula nosso erro E.</p><p id="cbe8" class="graf graf--p graf-after--p">Como algu&#xE9;m de olhos vendados em uma ladeira, podemos dar um passo e saber medir qual o efeito sobre a nossa altura <em class="markup--em markup--p-em">(subimos ou descemos, + ou -</em>), assim como a intensidade <em class="markup--em markup--p-em">(</em>magnitude num&#xE9;rica: 50cm ou 70 cm<em class="markup--em markup--p-em">)</em>. A partir da&#xED;, definimos uma regra para movimenta&#xE7;&#xE3;o.</p><figure id="b8fa" class="graf graf--figure graf-after--p"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/0*pjKOpe1SAyZeG_wf."></figure><p id="d4bf" class="graf graf--p graf-after--figure">Quando treinamos um &#xFA;nico nodo (SVM), o nosso trabalho &#xE9; como o de um cego tateando at&#xE9; descer ao lugar mais baixo. &#xC9; poss&#xED;vel seguir o caminho aos poucos. Com redes profundas, a entrada de um nodo depende da sa&#xED;da dos que se conectam a ele. O sistema &#xE9; um pouco mais complexo.</p><p id="db2b" class="graf graf--p graf-after--p">Vamos usar <strong class="markup--strong markup--p-strong">derivadas. </strong>Ou seu equivalente para fun&#xE7;&#xF5;es de m&#xFA;ltiplas vari&#xE1;veis, <strong class="markup--strong markup--p-strong">gradiente</strong>. O gradiente &#xE9; um vetor/lista com as derivadas parciais daquela fun&#xE7;&#xE3;o.</p><p id="9e68" class="graf graf--p graf-after--p">Matematicamente, queremos a derivada parcial da fun&#xE7;&#xE3;o de custo (f) com respeito &#xE0;s entradas. Como vimos, podemos encarar a rede neural como uma sequ&#xEA;ncia de fun&#xE7;&#xF5;es plugadas. Se o primeiro n&#xF3; tem q(x,y), o segundo, f, tem valor f(q(x,y) ou <em class="markup--em markup--p-em">f</em> o <em class="markup--em markup--p-em">q.</em></p><p id="0315" class="graf graf--p graf-after--p">q(x,y) = 3x+2y #camada inferior<br>f(z) = z&#xB2; #camada superior</p><p id="67b9" class="graf graf--p graf-after--p">f(q(x,y)) = q&#xB2; = (3x+2y)&#xB2; # input inferior para superior</p><p id="5743" class="graf graf--p graf-after--p">Podemos calcular o efeito de mudan&#xE7;as inter nodos com <strong class="markup--strong markup--p-strong">a regra de cadeia </strong>fun&#xE7;&#xF5;es compostas. Isto &#xE9;, podemos obter o gradiente de erro no nodo de hierarquia mais alta (f), com respeito a uma das vari&#xE1;veis de entrada (x) na hierarquia mais baixa. A opera&#xE7;&#xE3;o &#xE9; computacionalmente barata, bastando multiplicar as derivadas parciais dos erros em cada parte.</p><figure id="49a6" class="graf graf--figure graf-after--p"><img class="graf-image" src="https://cdn-images-1.medium.com/max/1600/1*Coy5kJUZlg7JY3G5Gzs4eA.png"></figure><p id="4685" class="graf graf--p graf-after--figure">&#xC9; poss&#xED;vel calcular de forma recursiva, portanto local e paralela, ao longo das camadas. Fazendo o mesmo acima para df/dy, teremos os valores de [df/dx&#xA0;; df/dy] que &#xE9; precisamente nosso gradiente.</p><pre id="f648" class="graf graf--pre graf-after--p"># Valor duplo (x,y) para inputs<br>x=1 <br>y=3</pre><pre id="643d" class="graf graf--pre graf-after--pre">q = 3*x + 2*y # primeira camada<br>f = q^2 # segunda camada</pre><pre id="766e" class="graf graf--pre graf-after--pre"># Backprop - Mudan&#xE7;as em hierarquia superior<br># dadas por entradas de camadas inferires<br>dfdq = 2*q # derivada de x&#xB2; ; varia&#xE7;&#xE3;o de f em fun&#xE7;&#xE3;o de q<br>dqdx = 3 # Derivada de 3x ; varia&#xE7;&#xE3;o de q em fun&#xE7;&#xE3;o de x<br>dqdy = 2 # Derivada de 2x ; varia&#xE7;&#xE3;o de q em fun&#xE7;&#xE3;o de y</pre><pre id="aa24" class="graf graf--pre graf-after--pre"># Obter gradiente de f(x,y) multiplicando as parciais <br>dfdx = dfdq*dqdx <br>dfdy = dfdq*dqdy<br>grad = c(dfdx,dfdy)<br>&gt; grad<br>[1] 24 16</pre><p id="798d" class="graf graf--p graf-after--pre">Usando essa l&#xF3;gica, calculamos os gradientes para a fun&#xE7;&#xE3;o de erro e treinamos o modelo.</p><p id="4846" class="graf graf--p graf-after--h3">Nos pr&#xF3;ximos epis&#xF3;dios, vamos colocar a m&#xE3;o na massa em aplica&#xE7;&#xF5;es mais realistas onde apenas algoritmos sofisticados de <em class="markup--em markup--p-em">machine learning </em>conseguem bom desempenho.</p><p id="34c8" class="graf graf--p graf-after--p graf--trailing">&#xB9; Para uma hist&#xF3;ria completa: <a href="http://www.idsia.ch/~juergen/deep-learning-overview.html" class="markup--anchor markup--p-anchor">J. Schmidhuber. Deep Learning in Neural Networks: An Overview.</a> Neural Networks, 61, p 85&#x2013;117, 2015. (Based on 2014 TR with 88 pages and 888 references, with PDF &amp; LATEX source &amp; complete public BIBTEX file).</p></div></div></div></div>
