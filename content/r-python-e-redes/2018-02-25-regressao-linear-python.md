+++
title = "Regressão Linear com Python"
date = "2018-02-25 03:00:00"
categories = ["r-python-e-redes"]
original_url = "http://neylsoncrepalde.github.io/2018-02-25-regressao-linear-python/"
+++

<article class="blog-post">
<p>
Olá. Hoje vamos revisar como estimar um modelo de regressão linear por
MQO no Python. Para isso, vamos usar <em>pandas</em>, <em>scipy</em> e a
biblioteca <em>statsmodels</em>. Há algumas outras bibliotecas para
estimação de modelos estatísticos em Python mas considero
<em>statsmodels</em> a melhor delas pela facilidade e praticidade de
uso. Outras bibliotecas como <em>scikit-learn</em> exigem um pouco mais
de linhas de programação e, consequentemente, uma base matemática mais
consolidada. Aliás, este é um ótimo exercício para quem está estudando
modelos lineares: tente programá-lo no Python sem a ajuda dos comandos
do pacote, <em>from scratch</em>. Sua compreensão vai melhorar bastante.
</p>
<p>
Antes de avançarmos para o código, vamos fazer uma breve revisão sobre
os modelos lineares por MQO.
</p>
<h2 id="regressão-linear-por-mínimos-quadrados-ordinários-mqo">
Regressão Linear por mínimos quadrados ordinários (MQO)
</h2>
<p>
A regressão linear é um método de análise estatística que nos permite
estimar o valor de uma determinada variável resposta (variável
dependente) como função de outras variáveis preditoras (variáveis
independentes). Essa relação pode ser expressa na seguinte equação:
</p>
<p>
<img src="http://neylsoncrepalde.github.io/img/2018-02-25-regressao-linear-python/equacao_regressao.png" alt="png">
</p>
<p>
onde <em>yi</em> representa a variável dependente, <em>xi1</em>,
<em>xi2</em>, <em>xip</em> representam as variáveis independentes, os
preditores da regressão, <em>b0</em> representa o <em>intercepto</em> da
regressão, ou seja, o valor esperado de <em>y</em> quando todos os
preditores tem valor = 0, <em>b1</em>, <em>b2</em>, <em>bp</em>
representam os coeficientes estimados da regressão e <em>epsilon</em>
representa o termo do erro, ou seja, a diferença entre os valores
esperados e os valores observados de <em>yi</em>.
</p>
<p>
O método de estimação dos <em>mínimos quadrados ordinários</em> - MQO
(do inglês <em>ordinary least squares</em> - OLS) é uma técnica de
otimização que busca o melhor ajuste do modelo através da minimização
dos quadrados do erro da regressão. A regressão linear é um método de
análise bastante poderoso. Contudo, ela está ancorada em alguns
pressupostos que precisam ser respeitados. Caso contrário, seus
resultados tendem a ficar enviesados. São eles:
</p>
<ol>
<li>
<p>
<strong>A relação entre a variável resposta e os preditores é
linear.</strong>
</p>
</li>
<li>
<p>
<strong>A variável resposta possui distribuição normal.</strong> Os
testes estatísticos desse método são todos baseados no pressuposto da
normalidade. Caso tentemos analisar uma variável que não possui
distribuição normal com esse método, os resultados não serão bons.
</p>
</li>
<li>
<p>
<strong>O termo do erro é não correlacionado com a variável
resposta.</strong>
</p>
</li>
<li>
<p>
<strong>O termo do erro possui distribuição normal com média 0.</strong>
</p>
</li>
<li>
<p>
<strong>O termo do erro é homoscedástico</strong>, ou seja, possui
variância constante em toda a sua extensão.
</p>
</li>
<li>
<p>
<strong>Os parâmetros populacionais são constantes,</strong> ou seja,
valores fixos desconhecidos os quais serão estimados pela equação de
regressão.
</p>
</li>
</ol>
<h2 id="estimando-mqo-no-python">
Estimando MQO no Python
</h2>
<p>
Vamos aos códigos. Primeiro, vamos gerar algumas variáveis aleatórias
distribuídas normalmente, uma variável categórica binária e um erro
normalmente distribuído para servir de banco de dados para nossa
regressão.
</p>
<pre class="highlight"><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="n">sp</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="k">as</span> <span class="n">sm</span> <span class="n">x1</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">x4</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">categoria</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">erro</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre>

<p>
Em seguida, vamos construir a variável resposta com parâmetros
conhecidos para fins de comparação. A equação verdadeira pode ser
expressa assim:
</p>
<p>
y = 3 + 2(x1) + 0.65(x2) + 3.14(x3) - 1.75(x4) + 1.5(dummy) + epsilon
</p>
<p>
Vamos verificar, no final deste tutorial, o desempenho da análise de
regressão em relação aos parâmetros verdadeiros.
</p>
<pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x1</span> <span class="o">+</span> <span class="mf">0.65</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="mf">3.14</span><span class="o">*</span><span class="n">x3</span> <span class="o">-</span> <span class="mf">1.75</span><span class="o">*</span><span class="n">x4</span> <span class="o">+</span> <span class="mf">1.5</span><span class="o">*</span><span class="n">categoria</span> <span class="o">+</span> <span class="n">erro</span>
</code></pre>

<p>
Em seguida, vamos criar um DataFrame com essas variáveis.
</p>
<pre class="highlight"><code><span class="n">dic</span> <span class="o">=</span> <span class="p">{</span><span class="s">&quot;y&quot;</span><span class="p">:</span><span class="n">y</span><span class="p">,</span> <span class="s">&quot;x1&quot;</span><span class="p">:</span><span class="n">x1</span><span class="p">,</span> <span class="s">&quot;x2&quot;</span><span class="p">:</span><span class="n">x2</span><span class="p">,</span> <span class="s">&quot;x3&quot;</span><span class="p">:</span><span class="n">x3</span><span class="p">,</span> <span class="s">&quot;x4&quot;</span><span class="p">:</span><span class="n">x4</span><span class="p">,</span> <span class="s">&quot;categoria&quot;</span><span class="p">:</span><span class="n">categoria</span><span class="p">}</span>
<span class="n">dados</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">dic</span><span class="p">)</span>
<span class="n">dados</span> <span class="c">#Visualiza o DataFrame</span>
</code></pre>

<table class="dataframe">
<thead>
<tr>
<th>
</th>
<th>
categoria
</th>
<th>
x1
</th>
<th>
x2
</th>
<th>
x3
</th>
<th>
x4
</th>
<th>
y
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
0
</td>
<td>
-1.027964
</td>
<td>
0.005811
</td>
<td>
0.735222
</td>
<td>
0.613691
</td>
<td>
2.028470
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1
</td>
<td>
1.193090
</td>
<td>
0.258837
</td>
<td>
-0.017317
</td>
<td>
-0.821739
</td>
<td>
8.377292
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1
</td>
<td>
1.225674
</td>
<td>
-2.640500
</td>
<td>
-0.889131
</td>
<td>
-2.218702
</td>
<td>
5.831579
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1
</td>
<td>
1.288136
</td>
<td>
0.915106
</td>
<td>
1.903477
</td>
<td>
0.568446
</td>
<td>
13.142979
</td>
</tr>
<tr>
<th>
4
</th>
<td>
0
</td>
<td>
-0.828994
</td>
<td>
0.148843
</td>
<td>
0.344978
</td>
<td>
0.051499
</td>
<td>
1.756007
</td>
</tr>
<tr>
<th>
5
</th>
<td>
0
</td>
<td>
-0.007645
</td>
<td>
-1.902100
</td>
<td>
0.942490
</td>
<td>
-0.322518
</td>
<td>
5.588279
</td>
</tr>
<tr>
<th>
6
</th>
<td>
1
</td>
<td>
0.088384
</td>
<td>
-2.736063
</td>
<td>
-0.215483
</td>
<td>
-0.588779
</td>
<td>
3.413264
</td>
</tr>
<tr>
<th>
7
</th>
<td>
0
</td>
<td>
-0.847198
</td>
<td>
-1.718300
</td>
<td>
-1.105616
</td>
<td>
-0.505273
</td>
<td>
-1.082864
</td>
</tr>
<tr>
<th>
8
</th>
<td>
0
</td>
<td>
-0.320159
</td>
<td>
1.756683
</td>
<td>
1.476829
</td>
<td>
-1.037843
</td>
<td>
8.531030
</td>
</tr>
<tr>
<th>
9
</th>
<td>
1
</td>
<td>
-0.487969
</td>
<td>
-1.084512
</td>
<td>
1.147471
</td>
<td>
0.394563
</td>
<td>
5.768816
</td>
</tr>
<tr>
<th>
10
</th>
<td>
0
</td>
<td>
-0.097839
</td>
<td>
-0.655643
</td>
<td>
-0.064105
</td>
<td>
-1.649199
</td>
<td>
5.679842
</td>
</tr>
<tr>
<th>
11
</th>
<td>
0
</td>
<td>
0.955571
</td>
<td>
-0.853956
</td>
<td>
0.749347
</td>
<td>
-0.794555
</td>
<td>
6.654270
</td>
</tr>
<tr>
<th>
12
</th>
<td>
1
</td>
<td>
-0.747153
</td>
<td>
0.861207
</td>
<td>
0.637005
</td>
<td>
0.879909
</td>
<td>
2.074678
</td>
</tr>
<tr>
<th>
13
</th>
<td>
1
</td>
<td>
1.184340
</td>
<td>
-0.820462
</td>
<td>
-0.798241
</td>
<td>
0.272143
</td>
<td>
2.424067
</td>
</tr>
<tr>
<th>
14
</th>
<td>
0
</td>
<td>
-0.941562
</td>
<td>
-0.717335
</td>
<td>
-1.962072
</td>
<td>
-2.679478
</td>
<td>
-2.208384
</td>
</tr>
<tr>
<th>
15
</th>
<td>
0
</td>
<td>
0.061954
</td>
<td>
1.247335
</td>
<td>
-2.059399
</td>
<td>
2.176928
</td>
<td>
-7.798604
</td>
</tr>
<tr>
<th>
16
</th>
<td>
0
</td>
<td>
1.671110
</td>
<td>
1.880848
</td>
<td>
-0.749895
</td>
<td>
-0.011972
</td>
<td>
5.476915
</td>
</tr>
<tr>
<th>
17
</th>
<td>
0
</td>
<td>
-0.720185
</td>
<td>
-0.048464
</td>
<td>
-3.682315
</td>
<td>
0.250626
</td>
<td>
-9.847082
</td>
</tr>
<tr>
<th>
18
</th>
<td>
1
</td>
<td>
-1.209025
</td>
<td>
0.594774
</td>
<td>
0.452681
</td>
<td>
-0.639228
</td>
<td>
6.160792
</td>
</tr>
<tr>
<th>
19
</th>
<td>
0
</td>
<td>
0.666450
</td>
<td>
1.340304
</td>
<td>
1.219634
</td>
<td>
0.508693
</td>
<td>
7.987290
</td>
</tr>
<tr>
<th>
20
</th>
<td>
1
</td>
<td>
-0.316183
</td>
<td>
0.288424
</td>
<td>
1.202293
</td>
<td>
-1.018137
</td>
<td>
10.800371
</td>
</tr>
<tr>
<th>
21
</th>
<td>
1
</td>
<td>
0.172340
</td>
<td>
-0.960865
</td>
<td>
0.552074
</td>
<td>
-0.132070
</td>
<td>
7.261349
</td>
</tr>
<tr>
<th>
22
</th>
<td>
0
</td>
<td>
0.447723
</td>
<td>
0.817908
</td>
<td>
-0.390960
</td>
<td>
-0.497449
</td>
<td>
3.733669
</td>
</tr>
<tr>
<th>
23
</th>
<td>
1
</td>
<td>
0.369514
</td>
<td>
1.160988
</td>
<td>
-0.036910
</td>
<td>
-1.412312
</td>
<td>
10.157134
</td>
</tr>
<tr>
<th>
24
</th>
<td>
1
</td>
<td>
0.978639
</td>
<td>
1.002424
</td>
<td>
-0.726037
</td>
<td>
-0.222755
</td>
<td>
3.963156
</td>
</tr>
<tr>
<th>
25
</th>
<td>
0
</td>
<td>
-1.301225
</td>
<td>
0.381227
</td>
<td>
1.235124
</td>
<td>
2.317437
</td>
<td>
0.102828
</td>
</tr>
<tr>
<th>
26
</th>
<td>
0
</td>
<td>
-0.460727
</td>
<td>
0.415375
</td>
<td>
-1.344034
</td>
<td>
0.129310
</td>
<td>
-2.840350
</td>
</tr>
<tr>
<th>
27
</th>
<td>
0
</td>
<td>
-1.674894
</td>
<td>
-0.219167
</td>
<td>
-1.130843
</td>
<td>
-1.038059
</td>
<td>
-3.399894
</td>
</tr>
<tr>
<th>
28
</th>
<td>
0
</td>
<td>
0.601086
</td>
<td>
-0.880368
</td>
<td>
-0.937390
</td>
<td>
0.869635
</td>
<td>
0.578146
</td>
</tr>
<tr>
<th>
29
</th>
<td>
1
</td>
<td>
0.519467
</td>
<td>
0.449896
</td>
<td>
0.026430
</td>
<td>
1.029819
</td>
<td>
6.766147
</td>
</tr>
<tr>
<th>
...
</th>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
<td>
...
</td>
</tr>
<tr>
<th>
70
</th>
<td>
0
</td>
<td>
0.120537
</td>
<td>
-3.120954
</td>
<td>
1.473131
</td>
<td>
0.032333
</td>
<td>
6.469714
</td>
</tr>
<tr>
<th>
71
</th>
<td>
1
</td>
<td>
0.228711
</td>
<td>
-0.008374
</td>
<td>
1.309662
</td>
<td>
-0.660893
</td>
<td>
11.122802
</td>
</tr>
<tr>
<th>
72
</th>
<td>
1
</td>
<td>
-1.976085
</td>
<td>
1.199087
</td>
<td>
-0.390166
</td>
<td>
0.566129
</td>
<td>
-1.789751
</td>
</tr>
<tr>
<th>
73
</th>
<td>
0
</td>
<td>
0.211299
</td>
<td>
1.664436
</td>
<td>
-0.631966
</td>
<td>
-0.107809
</td>
<td>
3.539535
</td>
</tr>
<tr>
<th>
74
</th>
<td>
0
</td>
<td>
0.672581
</td>
<td>
2.205682
</td>
<td>
0.844343
</td>
<td>
1.581004
</td>
<td>
7.127058
</td>
</tr>
<tr>
<th>
75
</th>
<td>
1
</td>
<td>
-1.755712
</td>
<td>
-0.159558
</td>
<td>
-0.861777
</td>
<td>
0.688980
</td>
<td>
-1.826065
</td>
</tr>
<tr>
<th>
76
</th>
<td>
0
</td>
<td>
0.576626
</td>
<td>
0.127073
</td>
<td>
0.850716
</td>
<td>
0.334624
</td>
<td>
6.482678
</td>
</tr>
<tr>
<th>
77
</th>
<td>
1
</td>
<td>
-0.173947
</td>
<td>
-0.407799
</td>
<td>
1.277835
</td>
<td>
-0.174108
</td>
<td>
7.722501
</td>
</tr>
<tr>
<th>
78
</th>
<td>
1
</td>
<td>
1.793446
</td>
<td>
0.316700
</td>
<td>
1.076530
</td>
<td>
1.611143
</td>
<td>
9.772018
</td>
</tr>
<tr>
<th>
79
</th>
<td>
1
</td>
<td>
-0.558026
</td>
<td>
-0.178458
</td>
<td>
-0.761499
</td>
<td>
-0.015727
</td>
<td>
-0.297279
</td>
</tr>
<tr>
<th>
80
</th>
<td>
0
</td>
<td>
-0.682473
</td>
<td>
0.270851
</td>
<td>
-0.954590
</td>
<td>
-0.484107
</td>
<td>
0.030462
</td>
</tr>
<tr>
<th>
81
</th>
<td>
0
</td>
<td>
-0.129743
</td>
<td>
1.089140
</td>
<td>
-0.638405
</td>
<td>
1.458407
</td>
<td>
-1.674821
</td>
</tr>
<tr>
<th>
82
</th>
<td>
0
</td>
<td>
0.592878
</td>
<td>
-0.235288
</td>
<td>
0.035668
</td>
<td>
-0.887291
</td>
<td>
5.730212
</td>
</tr>
<tr>
<th>
83
</th>
<td>
1
</td>
<td>
-0.845119
</td>
<td>
-0.332484
</td>
<td>
1.361238
</td>
<td>
0.195806
</td>
<td>
8.018649
</td>
</tr>
<tr>
<th>
84
</th>
<td>
0
</td>
<td>
-1.032762
</td>
<td>
0.357636
</td>
<td>
0.218133
</td>
<td>
-1.539139
</td>
<td>
3.772423
</td>
</tr>
<tr>
<th>
85
</th>
<td>
0
</td>
<td>
0.313567
</td>
<td>
0.156096
</td>
<td>
-0.538710
</td>
<td>
-0.090811
</td>
<td>
1.210189
</td>
</tr>
<tr>
<th>
86
</th>
<td>
0
</td>
<td>
-1.606484
</td>
<td>
1.537090
</td>
<td>
0.326825
</td>
<td>
-0.833555
</td>
<td>
2.607500
</td>
</tr>
<tr>
<th>
87
</th>
<td>
1
</td>
<td>
0.060540
</td>
<td>
1.158225
</td>
<td>
-1.465049
</td>
<td>
0.788696
</td>
<td>
-2.903798
</td>
</tr>
<tr>
<th>
88
</th>
<td>
0
</td>
<td>
-0.557101
</td>
<td>
1.614070
</td>
<td>
-2.133689
</td>
<td>
-0.114207
</td>
<td>
-3.198860
</td>
</tr>
<tr>
<th>
89
</th>
<td>
0
</td>
<td>
0.364759
</td>
<td>
-0.754128
</td>
<td>
-0.106209
</td>
<td>
-0.626787
</td>
<td>
4.098707
</td>
</tr>
<tr>
<th>
90
</th>
<td>
1
</td>
<td>
0.519074
</td>
<td>
-0.854893
</td>
<td>
1.054564
</td>
<td>
0.429074
</td>
<td>
6.553203
</td>
</tr>
<tr>
<th>
91
</th>
<td>
1
</td>
<td>
0.098454
</td>
<td>
1.064374
</td>
<td>
-0.140774
</td>
<td>
0.533403
</td>
<td>
5.680029
</td>
</tr>
<tr>
<th>
92
</th>
<td>
0
</td>
<td>
1.435576
</td>
<td>
-0.942384
</td>
<td>
-1.706739
</td>
<td>
0.765818
</td>
<td>
-0.339778
</td>
</tr>
<tr>
<th>
93
</th>
<td>
1
</td>
<td>
0.544879
</td>
<td>
-0.614113
</td>
<td>
-0.028148
</td>
<td>
-1.792886
</td>
<td>
8.194594
</td>
</tr>
<tr>
<th>
94
</th>
<td>
0
</td>
<td>
-0.835324
</td>
<td>
0.114021
</td>
<td>
0.304025
</td>
<td>
1.674463
</td>
<td>
-1.785285
</td>
</tr>
<tr>
<th>
95
</th>
<td>
1
</td>
<td>
-0.981402
</td>
<td>
0.348162
</td>
<td>
-0.006721
</td>
<td>
0.343610
</td>
<td>
1.657995
</td>
</tr>
<tr>
<th>
96
</th>
<td>
1
</td>
<td>
-0.275160
</td>
<td>
0.261869
</td>
<td>
-2.552706
</td>
<td>
-0.284243
</td>
<td>
-3.720913
</td>
</tr>
<tr>
<th>
97
</th>
<td>
1
</td>
<td>
0.104991
</td>
<td>
0.547673
</td>
<td>
0.886136
</td>
<td>
-1.270264
</td>
<td>
10.635857
</td>
</tr>
<tr>
<th>
98
</th>
<td>
1
</td>
<td>
0.006807
</td>
<td>
-0.489711
</td>
<td>
0.319738
</td>
<td>
1.301208
</td>
<td>
0.025958
</td>
</tr>
<tr>
<th>
99
</th>
<td>
1
</td>
<td>
-0.114278
</td>
<td>
-0.511798
</td>
<td>
0.248029
</td>
<td>
0.973711
</td>
<td>
3.129828
</td>
</tr>
</tbody>
</table>
<p>
100 rows × 6 columns
</p>

<p>
Prontinho. Agora, vamos dar uma olhada nas estatísticas descritivas e na
distribuição da variável resposta. Lembre-se de que ela deve possuir
distribuição normal.
</p>
<pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">&quot;Estat&#xED;sticas descritivas de y:&quot;</span><span class="p">)</span>
<span class="n">dados</span><span class="p">[</span><span class="s">&apos;y&apos;</span><span class="p">]</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre>

<pre class="highlight"><code>Estat&#xED;sticas descritivas de y: count 100.000000
mean 3.709140
std 4.417438
min -9.847082
25% 0.665959
50% 3.753046
75% 6.709719
max 13.142979
Name: y, dtype: float64
</code></pre>

<pre class="highlight"><code><span class="c"># Elabora um histograma</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&apos;red&apos;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&apos;Histograma da vari&#xE1;vel resposta&apos;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>

<p>
<img src="http://neylsoncrepalde.github.io/img/2018-02-25-regressao-linear-python/output_8_0.png" alt="png">
</p>
<p>
A distribuição da variável parece “bem normal”. Agora vamos estimar o
modelo e mostrar os resultados.
</p>
<pre class="highlight"><code><span class="n">reg</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s">&apos;y~x1+x2+x3+x4+categoria&apos;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dados</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</code></pre>

<table class="simpletable">
<caption>
OLS Regression Results
</caption>
<tr>
<th>
Dep. Variable:
</th>
<td>
y
</td>
<th>
R-squared:
</th>
<td>
0.959
</td>
</tr>
<tr>
<th>
Model:
</th>
<td>
OLS
</td>
<th>
Adj. R-squared:
</th>
<td>
0.957
</td>
</tr>
<tr>
<th>
Method:
</th>
<td>
Least Squares
</td>
<th>
F-statistic:
</th>
<td>
442.1
</td>
</tr>
<tr>
<th>
Date:
</th>
<td>
Sun, 25 Feb 2018
</td>
<th>
Prob (F-statistic):
</th>
<td>
1.17e-63
</td>
</tr>
<tr>
<th>
Time:
</th>
<td>
12:48:30
</td>
<th>
Log-Likelihood:
</th>
<td>
-132.89
</td>
</tr>
<tr>
<th>
No. Observations:
</th>
<td>
100
</td>
<th>
AIC:
</th>
<td>
277.8
</td>
</tr>
<tr>
<th>
Df Residuals:
</th>
<td>
94
</td>
<th>
BIC:
</th>
<td>
293.4
</td>
</tr>
<tr>
<th>
Df Model:
</th>
<td>
5
</td>
<th>
</th>
<td>
</td>
</tr>
<tr>
<th>
Covariance Type:
</th>
<td>
nonrobust
</td>
<th>
</th>
<td>
</td>
</tr>
</table>
<table class="simpletable">
<tr>
<td>
</td>
<th>
coef
</th>
<th>
std err
</th>
<th>
t
</th>
<th>
P&gt;|t|
</th>
<th>
\[0.025
</th>
<th>
0.975\]
</th>
</tr>
<tr>
<th>
Intercept
</th>
<td>
2.6426
</td>
<td>
0.154
</td>
<td>
17.184
</td>
<td>
0.000
</td>
<td>
2.337
</td>
<td>
2.948
</td>
</tr>
<tr>
<th>
x1
</th>
<td>
2.0440
</td>
<td>
0.100
</td>
<td>
20.403
</td>
<td>
0.000
</td>
<td>
1.845
</td>
<td>
2.243
</td>
</tr>
<tr>
<th>
x2
</th>
<td>
0.7448
</td>
<td>
0.102
</td>
<td>
7.272
</td>
<td>
0.000
</td>
<td>
0.541
</td>
<td>
0.948
</td>
</tr>
<tr>
<th>
x3
</th>
<td>
3.0630
</td>
<td>
0.094
</td>
<td>
32.739
</td>
<td>
0.000
</td>
<td>
2.877
</td>
<td>
3.249
</td>
</tr>
<tr>
<th>
x4
</th>
<td>
-1.8298
</td>
<td>
0.102
</td>
<td>
-17.876
</td>
<td>
0.000
</td>
<td>
-2.033
</td>
<td>
-1.627
</td>
</tr>
<tr>
<th>
categoria
</th>
<td>
1.7417
</td>
<td>
0.193
</td>
<td>
9.014
</td>
<td>
0.000
</td>
<td>
1.358
</td>
<td>
2.125
</td>
</tr>
</table>
<table class="simpletable">
<tr>
<th>
Omnibus:
</th>
<td>
0.782
</td>
<th>
Durbin-Watson:
</th>
<td>
1.901
</td>
</tr>
<tr>
<th>
Prob(Omnibus):
</th>
<td>
0.676
</td>
<th>
Jarque-Bera (JB):
</th>
<td>
0.862
</td>
</tr>
<tr>
<th>
Skew:
</th>
<td>
-0.100
</td>
<th>
Prob(JB):
</th>
<td>
0.650
</td>
</tr>
<tr>
<th>
Kurtosis:
</th>
<td>
2.591
</td>
<th>
Cond. No.
</th>
<td>
3.31
</td>
</tr>
</table>
<p>
Vamos começar pela primeira tabela apresentada. As medidas mais
importantes para nós neste momento são o <em>R2</em> ajustado, a
estatística de teste F, o p-valor dessa estatística e, caso queiramos
comparar diferentes modelos, o log-likelihood, o <em>Akaike Information
Criterion</em> (AIC) e o <em>Bayesian Information Criterion</em> (BIC).
</p>
<p>
O <em>R2</em> ajustado nos dá uma medida de porcentagem da variância
explicada pelo modelo. Este modelo tem um poder explicativo de 95,7%.
Nada mau, hum? Mas não se engane! O <em>R2</em> não é uma medida de
ajuste mas apenas nos diz o quanto de nossos dados esse modelo explica.
O restante do que falta explicar é atribuído ao termo do erro, ou seja,
todas as outras coisas que tem influência sobre nossa variável resposta
mas que não estão no modelo ou não podem de alguma forma ser mensuradas.
Em ciências naturais é comum obtermos um <em>R2</em> próximo de 100%. Já
nas ciências sociais, costumamos ficar muito felizes quando conseguimos
30% da variância dos dados explicados. A natureza dos dados estudados é
bem diferente. A estatística de teste F e seu p-valor &lt; 0.001
basicamente nos mostram que esse modelo é estatisticamente válido.
</p>
<p>
Quanto aos coeficientes da regressão, os betas estimados, vejam os
resultados. Parecem bem confiáveis, não é mesmo? O modelo pegou muito
bem as rlações entre as variáveis e os valores verdadeiros estão todos
dentro do intervalo de confiança estimado. Podemos interpretá-los da
seguinte forma: quando todos os preditores são iguais a 0, o valor
esperado de y é 2,64. Para cada aumento de 1 unidade em x1, y aumenta,
em média, 2,04. Para cada aumento de 1 unidade em x2, y aumenta, em
média, 0,74. Para cada aumento de 1 unidade em x3, y aumenta, em média,
3,06. Para cada aumento de 1 unidade em x4, y <em>diminui</em> (veja o
sinal negativo no coeficiente), em média, 1,83. A variável categórica
possui uma análise diferente. Os casos da categoria 1 y têm um valor
esperado médio 1,74 maior do que os casos da categoria 0. O p-valor para
todas os coeficientes estimados é menor que 0.001 o que mostra que eles
são estatisticamente significativos.
</p>
<p>
Vamos agora plotar alguns gráficos de avaliação. Primeiro, vamos plotar
um histograma dos resíduos (<em>epsilon = y - ŷ</em>). Lembre-se de que
os resíduos precisam ter distribuição normal com média 0. Em seguida,
vamos plotar um gráfico de dispersão (<em>scatterplot</em>) dos dos
valores preditos (ŷ) e resíduos. É preciso que os resíduos e a variável
resposta sejam não correlacionados.
</p>
<pre class="highlight"><code><span class="n">y_hat</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&apos;orange&apos;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&apos;Histograma dos res&#xED;duos da regress&#xE3;o&apos;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>

<p>
<img src="http://neylsoncrepalde.github.io/img/2018-02-25-regressao-linear-python/output_12_0.png" alt="png">
</p>
<pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">res</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&apos;green&apos;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=-</span><span class="mi">10</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&apos;orange&apos;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&apos;$</span><span class="err">\</span><span class="s">epsilon = y - </span><span class="err">\</span><span class="s">hat{y}$ - Res&#xED;duos&apos;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&apos;$</span><span class="err">\</span><span class="s">hat{y}$ ou $E(y)$ - Predito&apos;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>

<p>
<img src="http://neylsoncrepalde.github.io/img/2018-02-25-regressao-linear-python/output_13_0.png" alt="png">
</p>
<p>
Agora, vamos observar apresentar apenas os coeficientes da regressão se
quisermos facilitar a análise e traçar a reta de regressão entre as
variáveis <em>y</em> e <em>x1</em>.
</p>
<pre class="highlight"><code><span class="n">coefs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
<span class="n">coefs</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">&apos;Coeficientes&apos;</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">coefs</span><span class="p">)</span>
</code></pre>

<pre class="highlight"><code> Coeficientes
Intercept 2.964863
x1 2.083678
x2 0.751478
x3 3.229035
x4 -1.880821
categoria 1.378816
</code></pre>

<pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">dados</span><span class="p">[</span><span class="s">&apos;y&apos;</span><span class="p">],</span> <span class="n">x</span><span class="o">=</span><span class="n">dados</span><span class="p">[</span><span class="s">&apos;x1&apos;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">&apos;blue&apos;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">X_plot</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">dados</span><span class="p">[</span><span class="s">&apos;x1&apos;</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="n">dados</span><span class="p">[</span><span class="s">&apos;x1&apos;</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">dados</span><span class="p">[</span><span class="s">&apos;x1&apos;</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_plot</span><span class="p">,</span> <span class="n">X_plot</span><span class="o">*</span><span class="n">reg</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">reg</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">&apos;r&apos;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">11</span><span class="p">,</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&apos;Reta de regress&#xE3;o&apos;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&apos;$y$ - Vari&#xE1;vel Dependente&apos;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&apos;$x1$ - Preditor&apos;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre>

<p>
<img src="http://neylsoncrepalde.github.io/img/2018-02-25-regressao-linear-python/output_16_0.png" alt="png">
</p>
</article>
<p class="blog-tags">
Tags: Python, Linear Regression, Data Science
</p>

