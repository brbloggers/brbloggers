<div><p class="text-muted text-uppercase mb-small text-right"> Por <a href="http://curso-r.com/author/caio">Caio</a> 17/02/2018 </p><div id="post-content"> <p>Caso voc&#xEA; j&#xE1; tenha se aventurado no mundo do web scraping, &#xE9; prov&#xE1;vel que tenha se deparado com um grande problema: volume. Muitas vezes, antes fazer uma an&#xE1;lise, precisamos scrapear um n&#xFA;mero colossal de p&#xE1;ginas at&#xE9; que tenhamos dados o suficiente para a nossa tarefa, n&#xFA;mero esse que chega a ser proibitivo a ponto de n&#xE3;o conseguirmos fazer aquilo que queremos.</p>
<p>Neste post vou explicar duas t&#xE9;cnicas para aumentar em dezenas de vezes a velocidade dos seus scrapers de forma que voc&#xEA; nunca mais precise de preocupar com a quantidade de dados necess&#xE1;ria para uma an&#xE1;lise.</p>
<div id="scrapers-sequenciais" class="section level2"> <p>Um scraper sequencial &#xE9; qualquer scraper que baixe uma p&#xE1;gina por vez, ou seja, que varra as p&#xE1;ginas em sequ&#xEA;ncia baixando uma a uma. Como veremos na se&#xE7;&#xE3;o a seguir isso n&#xE3;o &#xE9; muito eficiente, mas &#xE9; mesmo assim o que a maioria de n&#xF3;s faz.</p>
<p><strong>Nota:</strong> Nos exemplos que darei daqui em diante estarei baixando uma lista de 1441 artigos da Wikip&#xE9;dia obtida com o pacote <code>WikipediR</code>. Se voc&#xEA; quiser reproduzir os meus achados, disponibilizei um arquivo com o c&#xF3;digo completo em um <a href="https://gist.github.com/ctlente/84f230a88cae01537ac5ca4eff091221">Gist</a></p>
<p>Veja mais ou menos como funcionaria para baixar um link da Wikip&#xE9;dia por vez:</p>
<pre class="r"><code># Fun&#xE7;&#xE3;o para baixar uma p&#xE1;gina da Wikip&#xE9;dia
download_wiki &lt;- function(url, path) { # Converter um URL em um nome de arquivo file &lt;- url %&gt;% utils::URLdecode() %&gt;% stringr::str_extract(&quot;(?&lt;=/)[^/]+$&quot;) %&gt;% stringr::str_replace_all(&quot;[:punct:]&quot;, &quot;&quot;) %&gt;% stringr::str_to_lower() %&gt;% stringr::str_c(normalizePath(path), &quot;/&quot;, ., &quot;.html&quot;) # Salvar a p&#xE1;gina no disco httr::GET(url, httr::write_disk(file, TRUE)) return(file)
} # Baixar arquivos sequencialmente
files &lt;- purrr::map_chr(links, download_wiki, &quot;~/Desktop/Wiki&quot;)</code></pre>
<p>Nada muito complexo at&#xE9; a&#xED;. Com a <code>purrr::map_chr()</code> itero com facilidade nos links e os baixo sequencialmente (se voc&#xEA; quiser saber mais sobre a fun&#xE7;&#xE3;o <code>map()</code> veja <a href="http://ctlente.com/pt/purrr-magic/">este post</a>). O c&#xF3;digo acima demorou mais ou menos 5 minutos para executar na minha m&#xE1;quina.</p>
</div>
<div id="scrapers-em-paralelo" class="section level2"> <p>Uma das formas mais simples de aumentar a efici&#xEA;ncia de um web scraper &#xE9; atrav&#xE9;s de paraleliza&#xE7;&#xE3;o. Um fato que nem todos sabem &#xE9; que praticamente qualquer scraper passa a maior parte do tempo esperando respostas do servidor; seja para carregar uma nova p&#xE1;gina ou seja para baixar a p&#xE1;gina em quest&#xE3;o, ficar esperando &#xE9; o que o seu scraper provavelmente mais faz.</p>
<p>Isso quer dizer que seu computador poderia ter, em qualquer dado momento, m&#xFA;ltiplos scrapers rodando simultaneamente sem perder efici&#xEA;ncia! Enquanto o processador est&#xE1; salvando no disco os resultados de um scraper, &#xE9; perfeitamente poss&#xED;vel ter muitos outros ativos e esperando uma resposta do servidor.</p>
<p>No exemplo de c&#xF3;digo abaixo uso uma fun&#xE7;&#xE3;o muito simples para paralelizar a execu&#xE7;&#xE3;o do scraper. <code>parallel::mcmapply()</code> (<em>multicore mapply()</em>) &#xE9; an&#xE1;loga a <code>map()</code>, com a diferen&#xE7;a de que ela instancia as chamadas para a fun&#xE7;&#xE3;o <code>download_wiki()</code> em m&#xFA;ltiplos threads de execu&#xE7;&#xE3;o, tirando vantagem do fato de que cada inst&#xE2;ncia fica parada a maior parte do tempo.</p>
<pre class="r"><code># Criar uma vers&#xE3;o empacotada de download_wiki()
download_wiki_ &lt;- purrr::partial( download_wiki, path = &quot;~/Desktop/Wiki&quot;, .first = FALSE) # Baixar arquivos em paralelo
files &lt;- parallel::mcmapply( download_wiki_, links, SIMPLIFY = TRUE, mc.cores = 4)</code></pre>
<p>No c&#xF3;digo acima, crio uma vers&#xE3;o pr&#xE9;-preenchida de <code>download_wiki()</code> para n&#xE3;o precisar lidar com argumentos constantes na chamada para <code>parallel::mcmapply()</code>, mas depois disso a &#xFA;nica coisa que preciso fazer &#xE9; especificar o n&#xFA;mero de cores dispon&#xED;veis no meu computador para que o pacote <code>parallel</code> fa&#xE7;a a sua magia. Desta forma, com uma chamada marginalmente mais complexa, consegui baixar os mesmos arquivos em meros 1.2 minutos.</p>
</div>
<div id="scrapers-distribuidos" class="section level2"> <p>Para o nosso <em>grand finale</em> temos um pequeno salto de dificuldade. Agora que somos capazes de usar todo o potencial do nosso computador, a &#xFA;nica forma de fazer scraping mais r&#xE1;pido &#xE9; usando <strong>mais</strong> computadores.</p>
<p>Isso parece loucura, mas usando m&#xE1;quinas virtuais da <a href="https://aws.amazon.com/pt/ec2/">Amazon</a> ou da <a href="https://cloud.google.com/compute/">Google</a> essa &#xE9; na verdade uma tarefa bastante simples! Podemos criar algumas inst&#xE2;ncias virtuais e enviar os links para que elas os baixem, distribuindo o download entre v&#xE1;rias m&#xE1;quinas.</p>
<div class="figure">
<img src="http://curso-r.com/blog/2018-02-17-scraper-distribuido/scheme.png"> </div>
<p>Para permitir que uma m&#xE1;quina virtual receba o comando de download, criei um pequeno servidor HTTP em cada uma, assim elas ficar&#xE3;o esperando por uma chamada POST contendo os URLs a serem baixados.</p>
<pre class="python"><code># Trecho do c&#xF3;digo em python do servidor que lida com POSTs
def do_POST(self): content_length = int(self.headers[&apos;Content-Length&apos;]) post_data = self.rfile.read(content_length) call([&quot;Rscript&quot;, &quot;~/script.R&quot;, post_data])</code></pre>
<p>Como pode-se ver no c&#xF3;digo acima, a &#xFA;nica coisa que o servidor faz &#xE9; coletar os dados enviados pelo post e redirecion&#xE1;-los para o script <code>script.R</code>. L&#xE1; o R coleta os links vindos de <code>post_data</code> e os baixa (usando, &#xE9; claro, <code>parallel::mcmapply</code>).</p>
<pre class="r"><code>#!/usr/bin/env Rscript
args = commandArgs(trailingOnly=TRUE) # Tratar o pacote de dados enviado no POST
links &lt;- stringr::str_split(args[1], &quot; &quot;)[[1]]</code></pre>
<p>Acima temos a &#xFA;nica diferen&#xE7;a no c&#xF3;digo em R (que agora se encontra nas m&#xE1;quinas virtuais): o tratamento necess&#xE1;rio em <code>script.R</code> dos dados trazidos pela chamada POST.</p>
<p>O &#xFA;ltimo passo &#xE9;, em nossa m&#xE1;quina local, quebrar a lista de links em um pacote para cada m&#xE1;quina serva; assim que as m&#xE1;quinas receberem esses links via HTTP elas come&#xE7;ar&#xE3;o, distribuidamente, a baix&#xE1;-los em paralelo.</p>
<pre class="r"><code># Quebrar os links de acordo com o n&#xFA;mero de servos
num_workers &lt;- 3
links_split &lt;- links %&gt;% split(., ceiling(seq_along(.)/(length(.)/num_workers))) %&gt;% purrr::map(stringr::str_c, collapse = &quot; &quot;) # Dados do endpoint
workers &lt;- &quot;localhost&quot; # AQUI V&#xC3;O OS IPS DOS SERVOS
endpoints &lt;- stringr::str_c(&quot;http://&quot;, workers, &quot;:8000&quot;) # Chamar todos os servos mas n&#xE3;o esperar por eles
for (i in seq_len(num_workers)) { command &lt;- paste0(&quot;curl -d &apos;&quot;, links_split[[i]], &quot;&apos; &quot;, endpoints[i]) system(command, wait = FALSE)
}</code></pre>
<p>Usando 3 m&#xE1;quinas virtuais de 4 cores cada no Google Cloud Platform, o download das 1400 p&#xE1;ginas demorou meros 34 segundos. Isso &#xE9; uma melhora de aproximadamente 10 vezes na performance em rela&#xE7;&#xE3;o &#xE0; execu&#xE7;&#xE3;o sequencial!</p>
</div>
<div id="conclusao" class="section level2"> <p>Como vimos nos exemplos acima, scrapers s&#xE3;o por padr&#xE3;o processos lentos e ineficientes. Usando uma arquitetura razoavelmente simples distribu&#xED;da e paralela podemos aumentar em at&#xE9; uma ordem de grandeza a efici&#xEA;ncia de um scraper sem nem pensar sobre o seu c&#xF3;digo! Na pr&#xE1;tica, podemos aumentar e diminuir o quanto quisermos o n&#xFA;mero de servos ou de cores em cada servo, permitindo que qualquer scraper possa virar uma m&#xE1;quina incr&#xED;vel de coleta de dados.</p>
<p>Caso voc&#xEA; tenha se interessado pelo conte&#xFA;do abordado nesse post, eu e o pessoal da <a href="http://curso-r.com/">Curso-R</a> vamos dar no dia 10/03/2018 um workshop em S&#xE3;o Paulos sobre <a href="http://workshop.curso-r.com/web-scraping/">web scraping com R</a>. L&#xE1; voc&#xEA; vai ter a oportunidade de aprender, do zero, como fazer bons web scrapers em R al&#xE9;m de muitas dicas como a desse post para tornar seus scrapers ainda melhores.</p>
</div> </div></div>
