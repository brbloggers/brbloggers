+++
title = "Aquele 1% é Deep Learning - Gerando letras do Wesley Safadão"
date = "2017-08-28 20:26:00"
categories = ["curso-r"]
original_url = "http://curso-r.com/blog/2017/08/28/2017-08-27-safadao/"
+++

<p>
O trabalho foi feito em 3 passos: download, modelagem e implantação.
Descrevemos cada um dos passos a seguir.
</p>
<p>
As letras foram baixadas do
<a href="https://www.letras.mus.br/">letras.mus.br</a>. Primeiro,
rodamos um script que lista os links de todas as músicas a partir da
página do Wesley Safadão. O CSS path esquisito abaixo foi a forma mais
compacta que encontrei de acessar os links diretamente.
</p>
<pre class="r"><code>library(magrittr)
link_base &lt;- &apos;https://www.letras.mus.br&apos;
# listando os links
ws_links &lt;- paste0(link_base, &apos;/wesley-safadao/&apos;) %&gt;% rvest::html_session() %&gt;% rvest::html_nodes(&apos;.cnt-list--alp &gt; ul &gt; li &gt; a&apos;) %&gt;% rvest::html_attr(&apos;href&apos;)</code></pre>
<p>
Em seguida, criamos uma função que pega a letra a partir de uma página.
</p>
<pre class="r"><code>pegar_letra &lt;- function(link) { # do link at&#xE9; a parte que tem o conte&#xFA;do result &lt;- paste0(link_base, link) %&gt;% rvest::html_session() %&gt;% rvest::html_nodes(&apos;.cnt-letra &gt; article &gt; p&apos;) %&gt;% # Peguei o texto com as tags html para pegar os \n as.character() %&gt;% stringr::str_replace_all(&apos;&lt;[brp/]+&gt;&apos;, &apos;\n&apos;) %&gt;% paste(collapse = &apos;\n\n&apos;) %&gt;% # Limpeza do texto limpar_musica() %&gt;% tokenizers::tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE) c(result, &apos;@&apos;) # Adicionando @ no final
}</code></pre>
<p>
E usamos o maravilhoso combo <code>purrr::map</code> com
<code>progress::progress</code>, que já tem um
<a href="http://curso-r.com/blog/2017/04/10/2017-04-08-progress/">post
dedicado no nosso blog</a>.
</p>
<pre class="r"><code># baixando todas as listas
p &lt;- progress::progress_bar$new(total = length(ws_links))
ws_letras &lt;- unlist(purrr::map(ws_links, ~{ p$tick() pegar_letra(.x)
}))</code></pre>
<p>
Note que eu escondi de vocês a função <code>limpar\_musica()</code>.
Essa função aplica uma série de
<a href="http://material.curso-r.com/stringr/">expressões regulares</a>
para limpar os textos.
</p>
<pre class="r"><code>limpar_musica &lt;- function(txt) { txt %&gt;% stringr::str_trim() %&gt;% stringr::str_to_lower() %&gt;% stringr::str_replace_all(&apos;[^a-z0-9&#xEA;&#xE2;&#xF4;&#xE1;&#xE9;&#xED;&#xF3;&#xFA;&#xE3;&#xF5;&#xE0;&#xE7;;,!?: \n-]&apos;, &apos;&apos;) %&gt;% stringr::str_replace_all(&apos;[0-9]+x| bis&apos;, &apos;&apos;) %&gt;% stringr::str_replace_all(&apos;([ ,?!])+&apos;, &apos;\\1&apos;) %&gt;% stringr::str_replace_all(&apos; ([;,!?:-])&apos;, &apos;\\1&apos;) %&gt;% stringr::str_replace_all(&apos;\n{3,}&apos;, &apos;\n\n&apos;)
}</code></pre>
<p>
O resultado é o objeto <code>ws\_letras</code>: um vetor tamanho 557459
em que cada elemento é um caractere, que pode ser uma letra, número,
espaço e até uma pulada de linha. Cada música é separada pelo caractere
<code>@</code>. Aqui está a primeira delas:
</p>
<pre class="r"><code>cat(head(ws_letras, which(ws_letras == &apos;@&apos;)[1] - 1), sep = &apos;&apos;)
## assim &#xE9; o nosso amor
## io io io io io iooo
## 100 amor
## io io io io io iooo
## ## s&#xF3; a gente se olhar que cora&#xE7;&#xE3;o dispara
## e as bocas calam
## e o desejo fala por n&#xF3;s dois
## ## canalisando o nosso amor,
## nada se compara
## &#xE9; fogo &#xE9; tara,
## no antes durante e depois
## ## coisa rara bonito de ver
## o mundo p&#xE1;ra pra eu e voc&#xEA;
## &#xE9; um conto de fadas a nossa paix&#xE3;o
## duas vidas em um s&#xF3; cora&#xE7;&#xE3;o</code></pre>

<p>
Não vou entrar em detalhes na parte estatística, mas basicamente
utilizei uma rede LSTM (Long Short-Term Memory) e apenas uma camada
oculta, copiada covardemente de um
<a href="https://rstudio.github.io/keras/articles/examples/lstm_text_generation.html">código
feito pelo Daniel Falbel nos tutoriais do Keras para o R</a>. O modelo
serve para classificar <strong>caracteres</strong> (não palavras) e
considera uma janela de passado máximo de 40 caracteres para realizar
suas predições. Por esse motivo as letras geradas podem ter erros
gramaticais feios (e.g. palavras iniciadas em <code>ç</code>).
</p>
<p>
Por simplicidade, omiti o código que faz a preparação dos dados para
ajustar no keras. Assim que eu tiver mais domínio sobre LSTM e Recurrent
Neural Networks (RNNs) em geral farei um post dedicado.
</p>
<p>
A especificação do modelo é simples: i) adicionamos apenas uma camada
LSTM com 128 unidades, ii) adicionamos uma camada oculta com o número de
unidades igual ao total de caracteres distintos presentes no texto e
iii) aplicamos uma ativação <code>softmax</code>, que dá as
probabilidades de cada candidato a próximo caractere.
</p>
<p>
Consideramos como função de custo a <em>Categorical Cross Entropy</em>,
a mesma da regressão logística. Como otimizador usamos o Adam, que faz
basicamente uma descida de gradiente, mas aplica médias móveis com o
passo anterior e com a derivada obtida via <em>back propagation</em>,
realizando atualizações mais suaves.
</p>
<p>
No final, ajustamos o modelo com mini-batches de 256 observações e cinco
épocas. Isso significa que fazemos 5 passos gigantes da descida de
gradiente usando toda a base de dados, separados em diversos passinhos
com 256 observações cada.
</p>
<p>
Na prática, eu rodei o <code>fit</code> algumas vezes, reduzindo
manualmente a taxa de aprendizado <code>lr</code> para fazer um ajuste
mais fino. Cada época demorava aproximadamente 6 minutos no meu
notebook, que não tem GPU.
</p>
<pre class="r"><code>library(keras)
model &lt;- keras_model_sequential()
model %&gt;% layer_lstm(128, input_shape = c(maxlen, length(chars))) %&gt;% layer_dense(length(chars)) %&gt;% layer_activation(&quot;softmax&quot;)
# custo e otimizador
model %&gt;% compile( loss = &quot;categorical_crossentropy&quot;, optimizer = optimizer_adam(lr = 0.0001)
)
# ajuste
model %&gt;% fit( keras_data$X, keras_data$y, batch_size = 256, epochs = 5
)</code></pre>
<p>
Também temos duas funções interessantes a serem discutidas. A primeira é
a <code>sample\_mod()</code>, uma função que recebe as probabilidades de
cada letra e gera uma nova letra com essas probabilidades. O parâmetro
<code>diversity=</code> aumenta ou diminui manualmente todas essas
probabilidades, fazendo o modelo alterar um pouco seu comportamento.
Quando maior esse parâmetro, maior a chance de saírem caracteres
inesperados e, quanto menor, maior a chance de sair um texto
completamente repetitivo.
</p>
<pre class="r"><code>sample_mod &lt;- function(preds, diversity = 1) { preds &lt;- log(preds) / diversity exp_preds &lt;- exp(preds) preds &lt;- exp_preds / sum(exp_preds) which.max(as.integer(rmultinom(1, 1, preds)))
}</code></pre>
<p>
A outra função é <code>gerar\_txt()</code>, nosso gerador de textos.
Essa função recebe o modelo do Wesley Safadão e retorna um novo texto. O
algoritmo funciona assim:
</p>
<ol>
<li>
<strong>Posicionamento</strong>. Escolhemos aleatoriamente uma posição
do texto de entrada que tenha um <code>@</code>
(<code>start\_index</code>). Lembre-se, o <code>@</code> delimita o
final ou início de uma letra.
</li>
<li>
<strong>Inicialização</strong>. Pegamos os 40 caracteres seguintes,
indicados por <code>maxlen=</code> e guardamos no vetor
<code>sentence</code>.
</li>
<li>
<strong>Geração de caracteres</strong>. Em seguida, entramos no seguinte
laço: enquanto o modelo não gera um <code>@</code> (final da canção),
criamos um novo caractere com <code>sample\_mod()</code> e adicionamos à
nossa sentença final. Para garantir que o código termina de rodar num
tempo finito, paramos o laço se criarmos mais de <code>limit=</code> sem
aparecer um <code>@</code>.
</li>
<li>
<strong>Impressão</strong>. Na hora de imprimir o texto, adicionamos um
<code>|</code> como separador para indicar qual parte foi extraída da
base real e qual parte é gerada automaticamente. Também adicionamos um
<code>&lt;truncated&gt;</code> no final caso a fase anterior tenha
passado do <code>limit=</code>.
</li>
</ol>
<pre class="r"><code>gerar_txt &lt;- function(model, txt, diversity = 1.0, limit = 1000, maxlen = 40) { # parte 1 - posicionamento chars &lt;- sort(unique(txt)) txt_index &lt;- which(txt[-length(txt)] == &apos;@&apos;) start_index &lt;- sample(txt_index, size = 1) + 1L id_txt &lt;- which(txt_index == start_index) # parte 2 - inicializa&#xE7;&#xE3;o sentence &lt;- txt[start_index:(start_index + maxlen - 1)] generated &lt;- paste0(c(sentence, &apos;|&apos;), collapse = &quot;&quot;) next_char &lt;- &quot;&quot; total_chars &lt;- 0 # parte 3 - gera&#xE7;&#xE3;o de caracteres while (next_char != &apos;@&apos; &amp;&amp; total_chars &lt; limit) { x &lt;- sapply(chars, function(x) {as.integer(x == sentence)}) dim(x) &lt;- c(1, dim(x)) next_index &lt;- sample_mod(predict(model, x), diversity) next_char &lt;- chars[next_index] generated &lt;- paste0(generated, next_char, collapse = &quot;&quot;) sentence &lt;- c(sentence[-1], next_char) total_chars &lt;- total_chars + 1 } # parte 4 - impress&#xE3;o s_final &lt;- stringr::str_sub(generated, 1, -2) if (total_chars == limit) s_final &lt;- paste0(s_final, &apos;\n&lt;truncated&gt;&apos;) s_final
}</code></pre>

<p>
Para deixar o modelo acessível pela internet, utilizei o maravilhoso
<code>OpenCPU</code>. Trata-se de um pacote em R e também um software
para transformar códigos R em API. Basicamente, o que fazemos é:
</p>
<ol>
<li>
<strong>Criar um pacote</strong> do R com as funções que temos
interesse. No nosso caso, temos o pacote
<a href="https://github.com/jtrecenti/safadao"><code>safadao</code></a>,
que foi criado para guardar o modelo ajustado e a função que gera as
letras, definida acima.
</li>
<li>
<strong>Instalar o OpenCPU</strong> em um servidor na nuvem.
</li>
<li>
<strong>Informar ao OpenCPU</strong> que queremos servir um pacote
específico.
</li>
</ol>
<p>
Felizmente, só precisei realizar de fato o primeiro passo dessa lista. O
<code>Jeroen Ooms</code>, autor dessa solução, nos dá uma vantagem a
mais: ele <a href="https://www.opencpu.org/cloud.html">mantém um
servidor na nuvem</a> onde qualquer usuário pode subir seu próprio
pacote, totalmente de graça. Ou seja, podemos criar APIs com nossos
modelos preferidos, de graça e sem esforço. Acesse
<a href="https://www.opencpu.org/cloud.html">esse link</a> para
instruções mais detalhadas de como fazer a implantação.
</p>
<p>
No nosso caso, a API é acessível pelo link abaixo.
</p>
<pre><code>http://jtrecenti.ocpu.io/safadao/R/gen/json</code></pre>
<p>
Basta fazer uma requisição POST para esse link e ele retornará uma letra
do Wesley Safadão.
</p>

<ul>
<li>
Vimos aqui mais uma aplicação da estatística que parece um pouco fora da
caixa mas que na verdade é bem pé no chão.
</li>
<li>
Para trabalhar com esse tipo de dados, usualmente usamos redes neurais
LSTM, adequada para dados em sequência.
</li>
<li>
O modelo ainda tem muito a melhorar, tanto com ajustes na modelagem
quanto na melhoria ao tratamento dos dados.
</li>
<li>
Agora você pode criar o gerador de músicas do seu artista preferido.
Tente replicar para outro artista!
</li>
</ul>
<p>
É isso. Happy coding ;)
</p>
<p>
PS: Também montei um <strong>gerador de salmos</strong> (da bíblia)
aleatório, usando a mesma técnica, mas ainda não estou feliz com o
resultado. Quando estiver, posto aqui também :P
</p>

